{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8860776e-64ba-43da-9a04-05563e7736e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "This section imports all necessary libraries for the project, including TensorFlow for the CNN, scikit-learn for traditional ML models, and other utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a374c-ccba-4828-bd7b-0b0974028b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edca6e1-2c57-4217-acea-80046267a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preparation\n",
    "This section loads the chest X-ray dataset, preprocesses the images (rescaling and resizing), and splits the data into training, validation, and test sets. A smaller subset is also created for initial experimentation with traditional ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b2da30-ae13-4e02-9a4c-3d0411296dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_dir = 'C:/ChestXRay/'\n",
    "train_dir = os.path.join(base_dir, 'Train')\n",
    "test_dir = os.path.join(base_dir, 'Test')\n",
    "\n",
    "# ImageDataGenerator for preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.15\n",
    ")\n",
    "\n",
    "# Training generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=64,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Validation generator\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=64,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Test generator\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=64,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get the full subset\n",
    "def get_subset(generator, num_samples=250):\n",
    "    X, y = [], []\n",
    "    for _ in range((num_samples // generator.batch_size) + 1):\n",
    "        images, labels = next(generator)\n",
    "        X.extend(images)\n",
    "        y.extend(labels)\n",
    "        if len(X) >= num_samples:\n",
    "            break\n",
    "    return np.array(X[:num_samples]), np.array(y[:num_samples])\n",
    "\n",
    "# Load the full subsets\n",
    "X_train_subset, y_train_subset = get_subset(train_generator, 250)\n",
    "X_val, y_val = get_subset(val_generator, 100)\n",
    "X_test, y_test = get_subset(test_generator, 584)\n",
    "\n",
    "print(f\"Training subset: {X_train_subset.shape}, Labels: {y_train_subset.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, Labels: {y_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, Labels: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795a7e3-55e6-4302-82ae-51b6bbc5235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class Distribution\n",
    "This section checks the class distribution to understand the imbalance between NORMAL and PNEUMONIA classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ffb846-8167-443b-8ac1-4acc840a5a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training set distribution\n",
    "train_classes = train_generator.classes\n",
    "print(\"Full training set distribution:\")\n",
    "print(f\"NORMAL (0): {np.sum(train_classes == 0)}, PNEUMONIA (1): {np.sum(train_classes == 1)}\")\n",
    "\n",
    "# Subset distribution\n",
    "print(\"250-image subset distribution:\")\n",
    "print(f\"NORMAL (0): {np.sum(y_train_subset == 0)}, PNEUMONIA (1): {np.sum(y_train_subset == 1)}\")\n",
    "\n",
    "# Test set distribution\n",
    "print(\"Test set distribution:\")\n",
    "print(f\"NORMAL (0): {np.sum(y_test == 0)}, PNEUMONIA (1): {np.sum(y_test == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9407b75-c931-4193-a4fa-b91f4d2685ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Traditional ML Models (SVM, Random Forest, KNN, Logistic Regression)\n",
    "This section trains four traditional ML models on a 250-image subset, with a focus on the SVM model. The SVM is selected as the best traditional model due to its perfect recall for PNEUMONIA, which is critical for medical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc7479d-8121-40a3-95bd-d419a0d50a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten images for traditional ML\n",
    "X_train_flat = X_train_subset.reshape(X_train_subset.shape[0], -1)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Class weights\n",
    "class_weights = {0: 1.0, 1: 5.0}\n",
    "\n",
    "# Define models (renamed to ml_models to avoid conflict)\n",
    "ml_models = {\n",
    "    'SVM': SVC(class_weight=class_weights, probability=True),\n",
    "    'Random Forest': RandomForestClassifier(class_weight=class_weights),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(class_weight=class_weights, max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "best_recall = 0\n",
    "best_model_name = None\n",
    "for name, model in ml_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train_flat, y_train_subset)\n",
    "    y_pred = model.predict(X_test_flat)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{name} - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    if rec > best_recall:\n",
    "        best_recall = rec\n",
    "        best_model_name = name\n",
    "        joblib.dump(model, f'best_model_{name}.pkl')\n",
    "\n",
    "print(f\"Best model based on recall: {best_model_name} with recall {best_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8defb3e0-4ca4-4e95-b8a6-b31783446305",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN on 250-Image Subset\n",
    "This section trains a basic CNN on a 250-image subset to test its performance before scaling to the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b241307-aba9-4f7a-863a-7933880239ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile with class weights\n",
    "cnn_model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.Recall()])\n",
    "\n",
    "# Train CNN\n",
    "history = cnn_model.fit(\n",
    "    X_train_subset, y_train_subset,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_recall = cnn_model.evaluate(X_test, y_test)\n",
    "print(f\"CNN - Test Accuracy: {test_acc:.4f}, Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "# Save if best recall\n",
    "if test_recall > best_recall:\n",
    "    cnn_model.save('best_cnn_model.h5')\n",
    "    print(\"CNN saved as best model based on recall\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28e0b8-8aad-4242-bc0d-0a1dcedfa18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full CNN on 4469 Images\n",
    "This section trains the CNN on the full training set (4469 images) to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b171458-ea8c-4e87-b85f-b5501a99bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new generators for the full training set\n",
    "full_train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=64,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "full_val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=64,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Define a new CNN model\n",
    "full_cnn_model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "full_cnn_model.compile(optimizer='adam',\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['accuracy', tf.keras.metrics.Recall()])\n",
    "\n",
    "# Train on the full training set\n",
    "history_full = full_cnn_model.fit(\n",
    "    full_train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=full_val_generator,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss_full, test_acc_full, test_recall_full = full_cnn_model.evaluate(test_generator)\n",
    "print(f\"Full CNN - Test Accuracy: {test_acc_full:.4f}, Test Recall: {test_recall_full:.4f}\")\n",
    "\n",
    "# Save if best recall\n",
    "if test_recall_full > best_recall:\n",
    "    full_cnn_model.save('best_full_cnn_model.h5')\n",
    "    print(\"Full CNN saved as best model based on recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19746cb5-759a-41fe-baf4-40cf3c3c997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adjust Threshold for Full CNN\n",
    "This section adjusts the prediction threshold of the full CNN to maximize recall for PNEUMONIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31c0fe-324e-4262-9447-88b09c50f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions with probabilities\n",
    "y_pred_prob = full_cnn_model.predict(test_generator)\n",
    "\n",
    "# Adjust threshold to 0.3 (instead of 0.5) to favor PNEUMONIA predictions\n",
    "threshold = 0.3\n",
    "y_pred_adjusted = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "# Calculate metrics with adjusted threshold\n",
    "acc_adjusted = accuracy_score(y_test, y_pred_adjusted)\n",
    "rec_adjusted = recall_score(y_test, y_pred_adjusted)\n",
    "prec_adjusted = precision_score(y_test, y_pred_adjusted)\n",
    "f1_adjusted = f1_score(y_test, y_pred_adjusted)\n",
    "\n",
    "print(f\"Full CNN (Adjusted Threshold {threshold}):\")\n",
    "print(f\"Accuracy: {acc_adjusted:.4f}, Precision: {prec_adjusted:.4f}, Recall: {rec_adjusted:.4f}, F1: {f1_adjusted:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86077a-f081-4933-82f9-0ff666fa7416",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the Full CNN Model\n",
    "This section saves the full CNN model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dfa03e-0835-46e1-812d-22038a1422d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions with probabilities\n",
    "y_pred_prob = full_cnn_model.predict(test_generator)\n",
    "\n",
    "# Adjust threshold to 0.3 (instead of 0.5) to favor PNEUMONIA predictions\n",
    "threshold = 0.3\n",
    "y_pred_adjusted = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "# Calculate metrics with adjusted threshold\n",
    "acc_adjusted = accuracy_score(y_test, y_pred_adjusted)\n",
    "rec_adjusted = recall_score(y_test, y_pred_adjusted)\n",
    "prec_adjusted = precision_score(y_test, y_pred_adjusted)\n",
    "f1_adjusted = f1_score(y_test, y_pred_adjusted)\n",
    "\n",
    "print(f\"Full CNN (Adjusted Threshold {threshold}):\")\n",
    "print(f\"Accuracy: {acc_adjusted:.4f}, Precision: {prec_adjusted:.4f}, Recall: {rec_adjusted:.4f}, F1: {f1_adjusted:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
